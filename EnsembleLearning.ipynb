{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn3iRHDiAPJo"
   },
   "source": [
    "#Ensemble Learning | Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBr-LnlxAU8-"
   },
   "source": [
    "# Question 1:  What is Ensemble Learning in machine learning Explain the key idea behind it.\n",
    "\n",
    "Ensemble Learning in **machine learning** is a technique where multiple models (often called *weak learners*) are trained and combined to solve the same problem. Instead of depending on a single model, ensemble methods bring together the predictions of several models to achieve better performance.\n",
    "\n",
    "### **Key Idea Behind Ensemble Learning**\n",
    "\n",
    "The central idea is that:\n",
    "\n",
    " *A group of models, when combined properly, can perform better than any individual model alone.*\n",
    "\n",
    "This works because:\n",
    "\n",
    "* Different models may make different errors.\n",
    "* By aggregating their predictions (through voting, averaging, or weighting), the errors can cancel out.\n",
    "* The ensemble becomes more accurate, more stable, and less prone to overfitting.\n",
    "\n",
    "### **Main Types of Ensemble Methods**\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):**\n",
    "\n",
    "   * Trains many models in parallel on random subsets of the data.\n",
    "   * Final prediction is made by majority voting (classification) or averaging (regression).\n",
    "   * Example: **Random Forest**.\n",
    "\n",
    "2. **Boosting:**\n",
    "\n",
    "   * Trains models sequentially. Each new model focuses on the mistakes of the previous ones.\n",
    "   * Example: **AdaBoost, XGBoost, LightGBM**.\n",
    "\n",
    "3. **Stacking:**\n",
    "\n",
    "   * Combines predictions of multiple models using another model (called a *meta-learner*) to learn the best way to blend them.\n",
    "\n",
    "---\n",
    "\n",
    "# Question 2: What is the difference between Bagging and Boosting?\n",
    "\n",
    "## **Bagging vs Boosting**:\n",
    "\n",
    "### **1. Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "* **Training style:** Models are trained **in parallel** on different random subsets of the training data (using sampling with replacement).\n",
    "* **Goal:** Reduce **variance** (helps prevent overfitting).\n",
    "* **How it works:** Each model votes/averages its prediction ‚Üí the ensemble output is more stable.\n",
    "* **Example:** **Random Forest**.\n",
    "\n",
    "### **2. Boosting**\n",
    "\n",
    "* **Training style:** Models are trained **sequentially**. Each new model focuses on the mistakes made by the previous models.\n",
    "* **Goal:** Reduce **bias** (helps improve accuracy on hard-to-predict cases).\n",
    "* **How it works:** Misclassified examples get more weight, so the next model learns them better. Final prediction is a weighted combination of all models.\n",
    "* **Example:** **AdaBoost, Gradient Boosting, XGBoost**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences (Summary Table)**\n",
    "\n",
    "| Aspect            | Bagging üß∫                      | Boosting üöÄ                         |\n",
    "| ----------------- | ------------------------------- | ----------------------------------- |\n",
    "| Training style    | Parallel                        | Sequential                          |\n",
    "| Focus             | Reduce **variance**             | Reduce **bias**                     |\n",
    "| Data sampling     | Random subsets with replacement | Weighted sampling (focus on errors) |\n",
    "| Model weight      | All models contribute equally   | Later models get higher weight      |\n",
    "| Example algorithm | Random Forest                   | AdaBoost, XGBoost                   |\n",
    "\n",
    "---\n",
    "\n",
    "# Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
    "\n",
    "## **Bootstrap Sampling**\n",
    "\n",
    "* **Definition:** Bootstrap sampling is a statistical technique where we create new datasets (called *bootstrap samples*) by **randomly selecting data points from the original dataset *with replacement***.\n",
    "* **With replacement** means: the same data point can appear multiple times in a sample, while some points may not appear at all.\n",
    "* Each bootstrap sample is usually the same size as the original dataset.\n",
    "\n",
    "## **Role in Bagging (e.g., Random Forest)**\n",
    "\n",
    "Bagging = **Bootstrap Aggregating** ‚Üí the name itself comes from bootstrap sampling!\n",
    "\n",
    "Here‚Äôs why it‚Äôs important:\n",
    "\n",
    "1. **Diversity of Models:**\n",
    "\n",
    "   * Each model (e.g., decision tree) is trained on a different bootstrap sample.\n",
    "   * This makes each model slightly different, even though they‚Äôre trained on the same overall dataset.\n",
    "\n",
    "2. **Reduced Variance:**\n",
    "\n",
    "   * Individual models might overfit their training data.\n",
    "   * But by training many models on varied bootstrap samples and averaging/voting their results, bagging reduces overfitting and variance.\n",
    "\n",
    "3. **Random Forest Specific:**\n",
    "\n",
    "   * In Random Forest, bootstrap sampling is used to build each decision tree.\n",
    "   * Additionally, Random Forest adds **feature randomness**: at each tree split, only a random subset of features is considered.\n",
    "   * This double randomness (data + features) makes the trees less correlated, improving ensemble performance.\n",
    "\n",
    "---\n",
    "\n",
    "# Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
    "\n",
    "## **Out-of-Bag (OOB) Samples**\n",
    "\n",
    "When we use **bootstrap sampling** in Bagging (like in Random Forest):\n",
    "\n",
    "* Each bootstrap sample is drawn **with replacement** from the original dataset.\n",
    "* On average, about **63% of the training data** ends up in a given bootstrap sample.\n",
    "* The remaining **\\~37% of data points** that were **not chosen** are called **Out-of-Bag (OOB) samples**.\n",
    "\n",
    "üëâ So, OOB samples are simply the data points **left out** when creating a bootstrap sample for training a particular model.\n",
    "\n",
    "\n",
    "## **OOB Score**\n",
    "\n",
    "The **OOB score** is a way to estimate the model‚Äôs performance **without needing a separate validation set**.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. Train each base model (e.g., each decision tree in Random Forest) on its bootstrap sample.\n",
    "2. For each data point in the dataset, look at the models where this point was **OOB** (i.e., not used for training).\n",
    "3. Use those models to predict the label for that data point.\n",
    "4. Compare predictions with the true labels ‚Üí compute accuracy (or other metric).\n",
    "\n",
    "This gives the **OOB score**, which is an **internal cross-validation estimate** of the ensemble‚Äôs generalization performance.\n",
    "\n",
    "\n",
    "## **Why OOB Score is Useful**\n",
    "\n",
    "* **No need for a separate validation set** ‚Üí we can use all data for training.\n",
    "* Provides an **unbiased estimate** of test error (similar to cross-validation).\n",
    "* Especially handy in Random Forests, where OOB scoring is commonly used as a quick measure of model accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "# Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
    "\n",
    "## **1. Feature Importance in a Single Decision Tree**\n",
    "\n",
    "* **How it‚Äôs measured:**\n",
    "  A tree decides splits based on some *impurity measure* (like Gini impurity, entropy for classification, or variance for regression).\n",
    "\n",
    "  * Each time a feature is used to split the data, the impurity reduction is recorded.\n",
    "  * The importance of a feature = sum of all impurity reductions from splits using that feature.\n",
    "  * Finally, the values are normalized (so they add up to 1).\n",
    "\n",
    "* **Pros:**\n",
    "\n",
    "  * Easy to compute and interpret.\n",
    "  * Shows how influential a feature was in building that tree.\n",
    "\n",
    "* **Cons:**\n",
    "\n",
    "  * **Unstable:** A single decision tree can vary a lot with small data changes.\n",
    "  * May give **biased importance** if features have many categories or scales.\n",
    "\n",
    "## **2. Feature Importance in a Random Forest**\n",
    "\n",
    "* **How it‚Äôs measured:**\n",
    "\n",
    "  * Each tree in the forest gives its own feature importance (same impurity reduction method).\n",
    "  * Random Forest averages these importances across all trees ‚Üí producing a more robust score.\n",
    "\n",
    "* **Alternative method (Permutation Importance):**\n",
    "\n",
    "  * Random Forests can also measure feature importance by randomly shuffling a feature and checking how much model accuracy drops.\n",
    "  * If accuracy falls a lot, that feature was important.\n",
    "\n",
    "* **Pros:**\n",
    "\n",
    "  * Much **more stable** and reliable because it averages across many trees.\n",
    "  * Captures importance even if features interact in complex ways.\n",
    "  * Permutation method avoids bias from categorical features.\n",
    "\n",
    "* **Cons:**\n",
    "\n",
    "  * Harder to interpret than a single tree.\n",
    "  * Importance may be ‚Äúdiluted‚Äù across correlated features (they share credit).\n",
    "\n",
    "\n",
    "## **Quick Comparison Table**\n",
    "\n",
    "| Aspect           | Single Decision Tree üå≥                        | Random Forest üå≤üå≤üå≤                           |\n",
    "| ---------------- | ---------------------------------------------- | ---------------------------------------------- |\n",
    "| Basis            | Impurity reduction in one tree                 | Average importance across many trees           |\n",
    "| Stability        | Unstable, sensitive to data                    | Stable, robust                                 |\n",
    "| Bias             | Can be biased toward features with many splits | Reduced bias, esp. with permutation importance |\n",
    "| Interpretability | Simple, easy to explain                        | More complex, aggregated importance            |\n",
    "| Use case         | Small/simple problems                          | Large, complex datasets                        |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mNd88OtCKNK"
   },
   "source": [
    "# Question 6: Write a Python program to:\n",
    "\n",
    "‚óè Load the Breast Cancer dataset using\n",
    "`sklearn.datasets.load_breast_cancer()`\n",
    "\n",
    "‚óè Train a Random Forest Classifier\n",
    "\n",
    "‚óè Print the top 5 most important features based on feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SfhC_oOCeF2",
    "outputId": "cc33a1d4-b5da-49c4-de6f-dee78cda64ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Important Features:\n",
      "\n",
      "             Feature  Importance\n",
      "          worst area    0.139357\n",
      "worst concave points    0.132225\n",
      " mean concave points    0.107046\n",
      "        worst radius    0.082848\n",
      "     worst perimeter    0.080850\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# 2. Train Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# 3. Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# 4. Create a DataFrame for better visualization\n",
    "feat_importances = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "})\n",
    "\n",
    "# 5. Sort and select top 5\n",
    "top5 = feat_importances.sort_values(by=\"Importance\", ascending=False).head(5)\n",
    "\n",
    "# Print results\n",
    "print(\"Top 5 Important Features:\\n\")\n",
    "print(top5.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cewION4eClnZ"
   },
   "source": [
    "# Question 7: Write a Python program to:\n",
    "\n",
    "‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
    "\n",
    "‚óè Evaluate its accuracy and compare with a single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxBi7XIqCgeW",
    "outputId": "ba70f39b-6659-479a-a470-14beb482edd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
      "Scikit-learn version: 1.6.1\n",
      "Accuracy of Single Decision Tree: 0.9333\n",
      "Accuracy of Bagging Classifier  : 0.9333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "import sys\n",
    "\n",
    "# Print versions for debugging\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 2. Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 3. Train a single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# 4. Train a Bagging Classifier with Decision Trees\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(), # Changed from base_estimator to estimator\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bag = bagging.predict(X_test)\n",
    "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
    "\n",
    "# 5. Print results\n",
    "print(\"Accuracy of Single Decision Tree: {:.4f}\".format(acc_dt))\n",
    "print(\"Accuracy of Bagging Classifier  : {:.4f}\".format(acc_bag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDBbuudNDGFG"
   },
   "source": [
    "#Question 8: Write a Python program to:\n",
    "\n",
    "‚óè Train a Random Forest Classifier\n",
    "\n",
    "‚óè Tune hyperparameters `max_depth` and `n_estimators` using GridSearchCV\n",
    "\n",
    "‚óè Print the best parameters and final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0wfK2-QC4EY",
    "outputId": "8f388ae6-05bb-4f8e-9954-291a3d1ec0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
      "Final Accuracy on Test Set: 0.9357\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 2. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 3. Define Random Forest and hyperparameter grid\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # number of trees\n",
    "    'max_depth': [None, 5, 10, 20]   # depth of trees\n",
    "}\n",
    "\n",
    "# 4. GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,              # 5-fold cross validation\n",
    "    n_jobs=-1,         # use all CPU cores\n",
    "    scoring='accuracy'\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. Best parameters and accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "final_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Final Accuracy on Test Set: {:.4f}\".format(final_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKw1Ou10DffB"
   },
   "source": [
    "#Question 9: Write a Python program to:\n",
    "\n",
    "‚óè Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
    "\n",
    "‚óè Compare their Mean Squared Errors (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-1lUruRDV58",
    "outputId": "697144c1-c895-4ef4-9b15-b5fff4524144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Bagging Regressor): 0.2568\n",
      "Mean Squared Error (Random Forest Regressor): 0.2565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# 2. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train Bagging Regressor with Decision Trees\n",
    "bagging = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(), # Changed from base_estimator to estimator\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bag = bagging.predict(X_test)\n",
    "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
    "\n",
    "# 4. Train Random Forest Regressor\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "# 5. Print results\n",
    "print(\"Mean Squared Error (Bagging Regressor): {:.4f}\".format(mse_bag))\n",
    "print(\"Mean Squared Error (Random Forest Regressor): {:.4f}\".format(mse_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDTEsnxkEnXL"
   },
   "source": [
    "#Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
    "\n",
    "You decide to use ensemble techniques to increase model performance.\n",
    "\n",
    "Explain your step-by-step approach to:\n",
    "\n",
    "  ‚óè Choose between Bagging or Boosting\n",
    "\n",
    "  ‚óè Handle overfitting\n",
    "\n",
    "  ‚óè Select base models\n",
    "\n",
    "  ‚óè Evaluate performance using cross-validation\n",
    "\n",
    "  ‚óè Justify how ensemble learning improves decision-making in this real-world context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bgr1yDeFHLT"
   },
   "source": [
    "## **Step 1: Choose Between Bagging or Boosting**\n",
    "\n",
    "**Factors to consider:**\n",
    "\n",
    "* **Data size:** Large datasets can benefit from **Bagging**, as it trains many models in parallel efficiently.\n",
    "* **Bias vs Variance:**\n",
    "\n",
    "  * **Bagging** reduces variance ‚Üí useful if individual models (like Decision Trees) tend to overfit.\n",
    "  * **Boosting** reduces bias ‚Üí useful if single models underfit and can learn sequentially from mistakes.\n",
    "* **Noise sensitivity:** Boosting can be sensitive to noisy data, which can lead to overfitting in financial datasets.\n",
    "\n",
    "**Decision:**\n",
    "\n",
    "* Start with **Bagging** (e.g., Random Forest) to get a robust baseline.\n",
    "* If underfitting is detected, try **Boosting** (e.g., XGBoost, LightGBM) to improve predictive power.\n",
    "\n",
    "## **Step 2: Handle Overfitting**\n",
    "\n",
    "**Techniques:**\n",
    "\n",
    "1. **Limit tree depth** (`max_depth`) and **minimum samples per leaf** to prevent overly complex trees.\n",
    "2. **Use ensemble averaging**: Bagging naturally reduces overfitting by averaging predictions.\n",
    "3. **Regularization in Boosting:** Parameters like `learning_rate` and `n_estimators` in XGBoost control overfitting.\n",
    "4. **Cross-validation**: Monitor performance on validation sets to detect overfitting early.\n",
    "\n",
    "## **Step 3: Select Base Models**\n",
    "\n",
    "* Common base models for ensemble techniques:\n",
    "\n",
    "  * **Decision Trees** ‚Üí widely used for Bagging and Boosting.\n",
    "  * **Logistic Regression** ‚Üí can be used in stacking ensembles for interpretability.\n",
    "  * **Other weak learners** like small neural networks or SVMs if boosting/staking is used.\n",
    "\n",
    "**Financial context tip:**\n",
    "\n",
    "* Decision Trees are preferred due to interpretability, which is important for **regulatory compliance** in financial institutions.\n",
    "\n",
    "## **Step 4: Evaluate Performance Using Cross-Validation**\n",
    "\n",
    "1. Split data into **k folds** (e.g., 5 or 10).\n",
    "2. Train ensemble models on **k-1 folds** and validate on the remaining fold.\n",
    "3. Metrics to monitor:\n",
    "\n",
    "   * **ROC-AUC** ‚Üí captures model ability to distinguish defaulters vs non-defaulters.\n",
    "   * **Precision/Recall** ‚Üí especially if default cases are rare (imbalanced data).\n",
    "   * **F1-Score** ‚Üí balances precision and recall.\n",
    "4. Average metrics across folds for a reliable performance estimate.\n",
    "\n",
    "**Optional:** Use **Stratified K-Fold** to ensure class proportions are maintained in each fold.\n",
    "\n",
    "## **Step 5: Justify How Ensemble Learning Improves Decision-Making**\n",
    "\n",
    "1. **Improved Accuracy:**\n",
    "\n",
    "   * Combining multiple models reduces variance and bias, leading to more reliable predictions of defaults.\n",
    "\n",
    "2. **Better Risk Assessment:**\n",
    "\n",
    "   * Ensemble models are more stable, reducing the likelihood of misclassifying high-risk customers.\n",
    "\n",
    "3. **Robustness to Noisy Data:**\n",
    "\n",
    "   * Bagging mitigates the effect of outliers in financial transactions.\n",
    "\n",
    "4. **Interpretability (with feature importance):**\n",
    "\n",
    "   * Even ensemble models like Random Forest provide feature importance scores to identify key predictors of default.\n",
    "\n",
    "5. **Regulatory and Business Confidence:**\n",
    "\n",
    "   * Financial institutions can explain decisions to stakeholders using ensemble-derived insights while maintaining high predictive performance.\n",
    "\n",
    "### **Step 6 (Optional Advanced Step): Stacking for Maximum Performance**\n",
    "\n",
    "* Combine multiple ensembles (e.g., Random Forest + XGBoost + Logistic Regression) with a **meta-model** to capture complementary strengths.\n",
    "* Helps improve predictions when different models capture different patterns in transaction history or demographics.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
